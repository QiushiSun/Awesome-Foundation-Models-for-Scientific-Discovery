# Underexplored Challenges in Protein Design

## Methodological and Computational Limitations

Protein design remains hampered by fundamental computational limits.  The sequence-to-structure search space is astronomically large and in practice NP-hard, so most methods rely on drastic approximations.  For example, design algorithms typically assume a fixed target fold and use simplified scoring functions to estimate folding energy, but balancing *accuracy* versus *speed* remains a major tradeoff.  As Jaramillo et al. note, scoring functions must “capture…relevant characteristics of the atomic interaction” yet permit fast computation – a compromise that inevitably omits important biophysical detail.  Similarly, traditional design frameworks largely ignore protein dynamics, intermediate folding states, and solvent effects, which limits their reliability.  On the experimental side, low‐throughput and noisy assays mean that only small variant libraries can be characterized, further constraining iterative design.  In short, both in silico and lab methods face scalability and accuracy ceilings that are often glossed over; overcoming them (for instance by better algorithms for coupled sequence/structure search or higher-throughput screening) is a recurring need identified by researchers.

## Biophysical Constraints and Complexities

Many designs still assume an idealized, static picture of proteins, overlooking complex biophysical realities.  Current models often ignore effects like protein conformational dynamics, cooperative multi‐residue interactions, or the role of bound water and ions.  For instance, standard energy functions neglect entropic contributions and solvent-mediated interactions, meaning that designed sequences may only be marginally stable or fold differently than predicted.  Moreover, proteins do not fold or act in dilute buffers in vivo: the intracellular milieu is densely crowded, highly heterogeneous, and chaperone-assisted, as underscored by Gershenson and Gierasch.  In-cell folding involves cotranslational folding, ribosome interactions, and quality-control machinery that are not captured by current design models.  This discrepancy is a largely overlooked constraint: proteins designed in isolation often fail when expressed in cells.  In future work, integrating more realistic biophysics (e.g. modeling co-translational folding or crowding effects) could improve design success in real biological contexts.

## Data Availability and Dataset Biases

A major bottleneck is the paucity and bias of available data.  Modern machine-learning design approaches require large, high-quality training datasets, but most experimentally assayed protein datasets are small or uneven.  As Ertelt *et al.* point out, there is a “mismatch between the limited data and the large demands of modern ML,” leading to poor generalization to new proteins.  Even consolidated resources (e.g. ProtaBank) are skewed – for example, they overrepresent destabilizing mutations or certain protein families – so models trained on them inherit systematic biases.  In practice, this means many ML-based design tools work well on test cases but fail when applied to truly novel targets.  Researchers therefore highlight the need for **more robust, standardized datasets** and benchmarks.  For example, Khakzad *et al.* note that most ML design studies are reported only as individual case studies and lack any common benchmarks, making objective evaluation difficult.  Building large-scale, unbiased mutation/folding datasets (and shared benchmarks) is an underexplored challenge that future work must address to ensure that design algorithms are reliable and comparable.

## Scalability and Generalization

Scaling designs to longer or more complex proteins remains difficult.  Most current methods have been demonstrated on small monomeric proteins or single chains; designing large proteins, multi-domain assemblies, or whole enzymes is still rare.  Even state-of-the-art structure predictors like AlphaFold face **sequence-length limits** (struggling with proteins >1,000 residues) that preclude efficient modeling of large complexes.  This computational bottleneck means many biologically important targets (large scaffolds, multimers, membrane complexes) are effectively out of reach.  On the generalization side, deep models trained on broad protein data often fail on specialized tasks.  For instance, Zheng *et al.* developed CrossDesign to show that standard “pan-protein” models lack functional specificity for enzymes, especially when structural data are scarce.  They report that limited structure–sequence data lead to undertrained models prone to overfitting, but that adapting pretrained language-model knowledge can improve out-of-domain performance.  In sum, current design methods generalize poorly to underrepresented protein classes or extreme conditions.  Future research may focus on *domain-adaptive* models and transfer learning to overcome data scarcity, and on hardware/software co-design (e.g. special accelerators) to break through scaling limits.

## Integration with Cellular Context

Protein design almost always assumes an isolated system, but real proteins operate in living cells.  Integration of design with cellular context is largely unaddressed.  In practice, factors like translation kinetics, post-translational modifications, protease targeting, immune recognition, and cellular compartmentalization can dramatically alter a protein’s behavior, yet they are seldom included in design pipelines.  Gershenson and Gierasch highlight that **in vitro** studies neglect issues like macromolecular crowding, cotranslational folding, and chaperones.  For example, a designed enzyme may fold and function in a test tube but misfold or degrade when expressed in *E. coli* or mammalian cells.  Recognizing this gap, researchers call for “context-aware” design strategies – for instance, coupling protein design to models of expression hosts or cellular networks.  Methods are just emerging to simulate proteostasis networks or incorporate evolutionary pressures at the organismal level, but much more work is needed.  Developing design tools that explicitly account for cellular factors (possibly by integrating systems-biology models or high-throughput in vivo assays) is a key direction for future research.

## Interpretability and Explainability of AI-driven Design

As machine learning plays a growing role, its **black-box** nature has become a critical concern.  Designed by neural networks, new proteins can be difficult to rationalize, and designers often cannot explain *why* a model chose a particular sequence.  Recent reviews stress that lack of transparency limits trust: “interpretability and transparency in these models \[are] \[..] challenges” that impede real-world application.  Medina-Ortiz *et al.* emphasize that explainable AI (XAI) methods have been applied in other biotech fields but remain almost **entirely underexplored in protein engineering**.  Without interpretability, it is hard to refine models or ensure they do not exploit spurious correlations.  Future work must therefore develop XAI techniques tailored to protein design – for example, identifying which residues or interactions most influence a model’s fitness predictions – and integrate them into design workflows.  Such explainability will make AI-designed proteins more reliable and may even suggest new biophysical insights.

**Sources:** Recent reviews and studies highlight these gaps.  For example, the need for benchmarks and standardization in ML design is noted by Khakzad *et al.*; dataset biases and generalization issues are discussed by Ertelt *et al.*; structural data scarcity is tackled by Zheng *et al.*; and the unmet challenge of XAI in protein engineering is explicitly stated by Medina-Ortiz *et al.*.  Addressing these underexplored challenges will be essential to advance protein design from promising lab techniques to robust, real-world engineering tools.
